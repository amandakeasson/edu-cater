{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edu-cater "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/amandae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "from scipy.io import savemat, loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # ???\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as pyldagensim\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import shortest_path\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class course_scraper():\n",
    "\n",
    "    def __init__(self): #thisworks\n",
    "        \n",
    "        self.level_dict = {'AllIntAdv': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Intermediate&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B1%5D=Advanced&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true',\n",
    "                           'AllMixed': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Mixed&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true',\n",
    "                           'AllBeg': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Beginner&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BentityTypeDescription%5D%5B0%5D=Courses&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Bskills%5D=&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true'}\n",
    "        \n",
    "        self.level_names = ['AllBeg', 'AllIntAdv', 'AllMixed']\n",
    "\n",
    "    def scrape_urls(self): #thisworks\n",
    "        urls_all = []\n",
    "        driver = webdriver.Chrome(\"/mnt/c/Users/easso/docs/neurohackademy/insight_examples/chromedriver.exe\")\n",
    "        for level_name in self.level_names:\n",
    "            print('Scraping', level_name, 'urls')\n",
    "            url = self.level_dict[level_name]\n",
    "            driver.get(url)\n",
    "            while True:\n",
    "                try:\n",
    "                    courses = driver.find_elements_by_xpath(\"//li[@class='ais-InfiniteHits-item']//a\")\n",
    "                    urls_page = [course.get_attribute(\"href\") for course in courses if \"/learn/\" in course.get_attribute(\"href\")]\n",
    "                    urls_all.extend(urls_page)\n",
    "                    button = driver.find_element_by_xpath(\"//button[@id='pagination_right_arrow_button' and @class='label-text box arrow']\")\n",
    "                    button.click()\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(\"Reached end of\", level_name, \"course list\")\n",
    "                    break\n",
    "\n",
    "        with open(\"edu-cater_urls.txt\", \"w\") as file:\n",
    "            for link in urls_all:\n",
    "                file.write(link + \"\\n\")\n",
    "                \n",
    "        file = open('edu-cater_urls.pkl', 'wb')\n",
    "        pickle.dump(urls_all, file)\n",
    "        file.close()\n",
    "    \n",
    "    def load_urls(self):\n",
    "        file = open('edu-cater_urls.pkl', 'rb')\n",
    "        self.urls_all = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    def scrape_courses(self):\n",
    "        file = open('edu-cater_urls.pkl', 'rb')\n",
    "        self.urls_all = pickle.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        # get course info\n",
    "        course_info_all = {}\n",
    "\n",
    "        for i, url in enumerate(self.urls_all):\n",
    "            print(url)\n",
    "            r  = requests.get(url)\n",
    "            data = r.text\n",
    "            soup = BeautifulSoup(data)\n",
    "\n",
    "            ### get course info and add to dictionary\n",
    "            # course title\n",
    "            title = soup.find(class_=\"H2_1pmnvep-o_O-weightNormal_s9jwp5-o_O-fontHeadline_1uu0gyz max-text-width-xl m-b-1s\").text\n",
    "            # course description\n",
    "            description = soup.find_all(class_='AboutCourse')[0].find(class_=\"content-inner\").text\n",
    "            # syllabus headings\n",
    "            syllabus_headings_all = soup.find_all(class_='H2_1pmnvep-o_O-weightBold_uvlhiv-o_O-bold_1byw3y2 m-b-2')\n",
    "            syllabus_headings = \"\"\n",
    "            for heading in syllabus_headings_all:\n",
    "                syllabus_headings += heading.text + \" \"\n",
    "            # syllabus descriptions\n",
    "            try:\n",
    "                syllabus_descriptions_all = soup.find_all(class_='Syllabus')[0].find_all(class_=\"content-inner\")\n",
    "                syllabus_descriptions = \"\"\n",
    "                for desc in syllabus_descriptions_all:\n",
    "                    syllabus_descriptions += desc.text + \" \"\n",
    "            except:\n",
    "                syllabus_descriptions = \"\"\n",
    "            # number of reviews\n",
    "            try:\n",
    "                nreviews = int(soup.find(itemprop=\"reviewCount\").text)\n",
    "            except:\n",
    "                nreviews = np.nan\n",
    "            # level\n",
    "            try:\n",
    "                level = soup.find('title', id=re.compile('Level')).text.split()[0]\n",
    "            except:\n",
    "                level = \"Mixed\"\n",
    "            # hours (course length)\n",
    "            try:\n",
    "                hours_tmp = soup.find_all(text=re.compile(\"Approx. \"))[0]\n",
    "                hours = int(hours_tmp.split('Approx. ')[1].split(' hours')[0])\n",
    "            except:\n",
    "                hours = np.nan\n",
    "            # stars (overall rating)\n",
    "            try:\n",
    "                stars = soup.find_all(class_=\"H4_1k76nzj-o_O-weightBold_uvlhiv-o_O-bold_1byw3y2 m-l-1s m-r-1 m-b-0\")\n",
    "                stars = float(stars[0].text)\n",
    "            except:\n",
    "                stars = np.nan\n",
    "            # enrollment\n",
    "            enrollment_tmp = soup.find('script', text = re.compile('totalEnrollment')).text\n",
    "            enrollment = int(enrollment_tmp.split('\"totalEnrollmentCount\":')[1].split('}')[0])\n",
    "            # skills you'll gain\n",
    "            skills = []\n",
    "            try:\n",
    "                soup.find_all(class_=\"Box_120drhm-o_O-displayflex_poyjc-o_O-wrap_rmgg7w\")[0].text\n",
    "                skills_tags = soup.find_all(class_=\"centerContent_dqfu5r\")\n",
    "                for skill in skills_tags:\n",
    "                    skills.append(skill.text)\n",
    "            except: \n",
    "                pass\n",
    "            # occupations (Learners taking this course are...)\n",
    "            occupations_tags = soup.find_all(class_=\"occupation-name\")\n",
    "            occupations = []\n",
    "            for occupation in occupations_tags:\n",
    "                occupations.extend(occupation)\n",
    "            # reviews\n",
    "            print(\"number of reviews:\", nreviews)\n",
    "            reviews = []\n",
    "            counter = 1\n",
    "            get_reviews = 0\n",
    "            if get_reviews==1:\n",
    "                while True:\n",
    "                    try:\n",
    "                        if counter%10==1:\n",
    "                            print(counter)\n",
    "                        if counter == 1:\n",
    "                            r = requests.get(url+'/reviews')\n",
    "                        else:\n",
    "                            r = requests.get(url+'/reviews'+'?page='+str(counter))\n",
    "                        data = r.text\n",
    "                        soup = BeautifulSoup(data)\n",
    "                        reviews_all = soup.find_all(class_=\"reviewText\")\n",
    "                        if len(reviews_all)==0:\n",
    "                            break\n",
    "                        else:\n",
    "                            for review in reviews_all:\n",
    "                                reviews.append(review.text)\n",
    "                            counter += 1\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # add info to dictionary\n",
    "            course_info  =   {'title': title,\n",
    "                              'description': description, \n",
    "                              'syllabus_headings': syllabus_headings,\n",
    "                              'syllabus_descriptions': syllabus_descriptions,\n",
    "                              'nreviews': nreviews,\n",
    "                              'level': level,\n",
    "                              'hours': hours,\n",
    "                              'stars': stars,\n",
    "                              'enrollment': enrollment,\n",
    "                              'skills': skills,\n",
    "                              'occupations': occupations,\n",
    "                              'reviews': reviews}\n",
    "            \n",
    "            # save course_info\n",
    "            file = 'course_info/course' + str(i) + '.json'\n",
    "            with open(file, 'w') as fp:\n",
    "                json.dump(course_info, fp)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def scrape_course_network(self):\n",
    "        file = open('edu-cater_urls.pkl', 'rb')\n",
    "        self.urls_all = pickle.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        # make course network\n",
    "        course_network = np.zeros((len(self.urls_all), len(self.urls_all)))\n",
    "\n",
    "        driver = webdriver.Chrome(\"/mnt/c/Users/easso/docs/neurohackademy/insight_examples/chromedriver.exe\")        \n",
    "        #for i, url in enumerate(self.urls_all):\n",
    "        for i in range(1302, len(self.urls_all)): #testcode\n",
    "            if i%100 == 0:\n",
    "                print(i) #testcode\n",
    "            url = self.urls_all[i] #testcode\n",
    "            print(url)\n",
    "            driver.get(url)\n",
    "            \n",
    "            recs_all = []\n",
    "            recs = driver.find_elements_by_xpath('//div[@class=\"m-a-1s\"]//div//a[@data-click-value]')\n",
    "            for rec in recs:\n",
    "                recs_all.append(rec.get_attribute(\"href\"))\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    button = driver.find_element_by_xpath(\"//button[@class='Button_1w8tm98-o_O-icon_1rbfoc-o_O-md_1jvotax']\")\n",
    "                except:\n",
    "                    break\n",
    "                button.click()\n",
    "                time.sleep(2)\n",
    "                recs = driver.find_elements_by_xpath('//div[@class=\"m-a-1s\"]//div//a[@data-click-value]')\n",
    "                repeats=0\n",
    "                for rec in recs:\n",
    "                    if rec.get_attribute(\"href\") in recs_all:\n",
    "                        repeats += 1\n",
    "                    else:\n",
    "                        recs_all.append(rec.get_attribute(\"href\"))\n",
    "                if repeats == len(recs):\n",
    "                    break\n",
    "\n",
    "            recs_all = np.unique(recs_all)\n",
    "            recs_all_courses = []\n",
    "            for rec in recs_all:\n",
    "                if '/learn/' in rec:\n",
    "                    recs_all_courses.append(rec)\n",
    "\n",
    "            recs_all_courses\n",
    "\n",
    "            for rec in recs_all_courses:\n",
    "                try:\n",
    "                    ind = scraper.urls_all.index(rec)\n",
    "                    course_network[i,ind] = 1\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "            coursenet = course_network[i,:]\n",
    "            savename = 'course_nets/course' + str(i) + '.mat'\n",
    "            savemat(savename,{'coursenet': coursenet})\n",
    "        return course_network\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.coursera.org/learn/musicianship-chords\n",
      "https://www.coursera.org/learn/cloud-applications-part1\n",
      "https://www.coursera.org/learn/data-collection-framework\n",
      "https://www.coursera.org/learn/conflict-resolution-mediation\n",
      "https://www.coursera.org/learn/federal-taxation-business\n",
      "https://www.coursera.org/learn/branding-and-cx\n",
      "https://www.coursera.org/learn/technical-writing\n",
      "https://www.coursera.org/learn/academic-literacy\n",
      "https://www.coursera.org/learn/international-negotiation\n",
      "https://www.coursera.org/learn/business-strategies\n",
      "https://www.coursera.org/learn/onprem-fundamentals-apigee-gcp\n",
      "https://www.coursera.org/learn/six-sigma-define-measure-advanced\n",
      "https://www.coursera.org/learn/javascript-jquery-json\n",
      "https://www.coursera.org/learn/robotics-perception\n",
      "https://www.coursera.org/learn/pathophysiology\n",
      "https://www.coursera.org/learn/advanced-r\n",
      "https://www.coursera.org/learn/agile-software-development\n",
      "https://www.coursera.org/learn/cataract-surgery\n",
      "https://www.coursera.org/learn/geometric-algorithms\n",
      "https://www.coursera.org/learn/microscopy\n",
      "https://www.coursera.org/learn/musicianship-harmony\n",
      "https://www.coursera.org/learn/patient-safety-systems-view\n",
      "https://www.coursera.org/learn/guitar-scales-chord-progressions\n",
      "https://www.coursera.org/learn/mobile-interaction-design\n",
      "https://www.coursera.org/learn/high-throughput\n",
      "https://www.coursera.org/learn/fusion-360-integrated-cad-cam-cae\n",
      "https://www.coursera.org/learn/management-leadership-english\n",
      "https://www.coursera.org/learn/reinforcement-learning-in-finance\n",
      "https://www.coursera.org/learn/leadership\n",
      "https://www.coursera.org/learn/survival-analysis-r-public-health\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "scraper = course_scraper() #thisworks\n",
    "#scraper.scrape_urls()      #thisworks\n",
    "#scraper.scrape_courses()\n",
    "scraper.load_urls()\n",
    "net = scraper.scrape_course_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing functions: tokenization, stemming, lemmatization \n",
    "# https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token!='youâ':\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all text info (description, syllabus) and preprocess\n",
    "\n",
    "processed_info = []\n",
    "nreviews = []\n",
    "stars = []\n",
    "hours = []\n",
    "levels = []\n",
    "enrollment = []\n",
    "course_info_all = []\n",
    "titles_all = []\n",
    "for i in range(2635): #fixthis\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "    with open('course_info_short/course' + str(i) + '.json') as json_file:\n",
    "        course_info = json.load(json_file)\n",
    "    allinfo = course_info['title'] + ' ' + course_info['description'] \\\n",
    "            + ' ' + course_info['syllabus_headings'] + ' ' + course_info['syllabus_descriptions'] \n",
    "    for skill in course_info['skills']:\n",
    "        allinfo = allinfo + ' ' + skill\n",
    "    for occupation in course_info['occupations']:\n",
    "        allinfo = allinfo + ' ' + occupation\n",
    "    for review in course_info['reviews']:\n",
    "        allinfo = allinfo + ' ' + review\n",
    "    nreviews.append(course_info['nreviews'])\n",
    "    stars.append(course_info['stars'])\n",
    "    hours.append(course_info['hours'])\n",
    "    levels.append(course_info['level'])\n",
    "    enrollment.append(course_info['enrollment'])\n",
    "    titles_all.append(course_info['title'])\n",
    "   \n",
    "    processed_info.append(preprocess(allinfo))\n",
    "    course_info_all.append(allinfo)\n",
    "    \n",
    "# save course titles for web app\n",
    "file = open('simple_app/course_titles.pkl', 'wb')\n",
    "pickle.dump(titles_all, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for quantitative info\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "fig.add_subplot(131)\n",
    "plt.hist(stars,20); plt.title('Rating')\n",
    "fig.add_subplot(132)\n",
    "plt.hist(hours,20); plt.title('Length (hours)')\n",
    "fig.add_subplot(133)\n",
    "plt.hist(np.log10(enrollment),20); plt.title('Enrollment (log10)')\n",
    "plt.savefig('ed1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_info)\n",
    "print('Length of originial dictionary:', len(dictionary))\n",
    "\n",
    "# remove rare and common words\n",
    "dictionary.filter_extremes(no_below=10, no_above=.25, keep_n=100000)\n",
    "print('Length of filtered dictionary:', len(dictionary))\n",
    "\n",
    "# make bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LDA model and print topics\n",
    "ntopics = 20\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = ntopics, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of best-matching topics\n",
    "\n",
    "bestmatch = np.argmax(scoremat,axis=1)\n",
    "plt.hist(bestmatch,20)\n",
    "my_x_labels = []\n",
    "for x in range(20):\n",
    "    my_x_labels.append('Topic ' + str(x+1))\n",
    "plt.xticks(np.arange(20),my_x_labels, rotation=90)\n",
    "plt.gcf().subplots_adjust(bottom=0.20)\n",
    "plt.title('Topic Distribution')\n",
    "plt.savefig('topicdist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unseen document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive visualization of topics with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "data = pyldagensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.save_html(data,'courseviz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity b/w topic scores \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "\n",
    "def doctopics(ind): \n",
    "    unseen_document = course_info_all[ind]\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "    topic_tuples = lda_model.get_document_topics(bow_vector, 0, 0, True)[0]\n",
    "    topic_scores = np.zeros((1, len(topic_tuples)))\n",
    "    for i, score in enumerate(topic_tuples):\n",
    "        topic_scores[0,i] = score[1]\n",
    "    return topic_scores\n",
    "    \n",
    "scoremat = np.zeros((len(course_info_all),ntopics))\n",
    "savemat('simple_app/scoremat.mat',{'scoremat': scoremat})\n",
    "\n",
    "for i in range(len(course_info_all)):\n",
    "    scoremat[i,:] = doctopics(i)\n",
    "\n",
    "thresh = .7\n",
    "scorecorrs = cos_sim(scoremat)\n",
    "scorecorrs[scorecorrs<=thresh] = 0\n",
    "scorecorrs[scorecorrs>thresh] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.imshow(scorecorrs, cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph theory with networkx\n",
    "G = nx.from_numpy_matrix(scorecorrs)\n",
    "weights = dict(G.degree(weight='weight'))\n",
    "values = [weights.get(node, 0.25) for node in G.nodes()]\n",
    "\n",
    "plt.figure(1,figsize=(12,12)) \n",
    "nx.draw(G, node_size=20, node_color = values, width=.1, cmap='plasma')\n",
    "plt.savefig(\"cousera_lda_network.png\", format=\"PNG\")\n",
    "plt.show(block=False)\n",
    "\n",
    "file = open('simple_app/networkx_graph.pkl','wb')\n",
    "pickle.dump(G, file)\n",
    "file.close()\n",
    "\n",
    "file = open('simple_app/networkx_values.pkl', 'wb')\n",
    "pickle.dump(values, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
