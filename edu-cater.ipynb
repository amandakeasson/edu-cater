{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edu-cater "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # ???\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as pyldagensim\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape course URLs\n",
    "\n",
    "level_dict = {'AllIntAdv': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Intermediate&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B1%5D=Advanced&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true',\n",
    "                'AllMixed': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Mixed&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true',\n",
    "                'AllBeg': 'https://www.coursera.org/search?query=%22%22&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Blanguage%5D%5B0%5D=English&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BproductDifficultyLevel%5D%5B0%5D=Beginner&indices%5Bprod_all_products%5D%5BrefinementList%5D%5BentityTypeDescription%5D%5B0%5D=Courses&indices%5Bprod_all_products%5D%5BrefinementList%5D%5Bskills%5D=&indices%5Bprod_all_products%5D%5Bpage%5D=1&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_all_products%5D%5Bconfigure%5D%5BhitsPerPage%5D=10&configure%5BclickAnalytics%5D=true'}\n",
    "\n",
    "\n",
    "# get all course URLs for a difficulty level and save to text file\n",
    "\n",
    "level_names = ['AllBeg', 'AllIntAdv', 'AllMixed']\n",
    "\n",
    "for level_name in level_names:\n",
    "    url = level_dict[level_name]\n",
    "\n",
    "    driver = webdriver.Chrome(\"/mnt/c/Users/easso/docs/neurohackademy/insight_examples/chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    urls_all = []\n",
    "    while True:\n",
    "        try:\n",
    "            courses = driver.find_elements_by_xpath(\"//li[@class='ais-InfiniteHits-item']//a\")\n",
    "            urls_page = [course.get_attribute(\"href\") for course in courses if \"/learn/\" in course.get_attribute(\"href\")]\n",
    "            urls_all.extend(urls_page)\n",
    "            button = driver.find_element_by_xpath(\"//button[@id='pagination_right_arrow_button' and @class='label-text box arrow']\")\n",
    "            button.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Reached end of course list\")\n",
    "            break\n",
    "\n",
    "    with open(level_name + \"_urls.txt\", \"w\") as file:\n",
    "        for link in urls_all:\n",
    "            file.write(link + \"\\n\")\n",
    "\n",
    "# combine into one file\n",
    "os.system('cat AllBeg_urls.txt AllIntAdv_urls.txt AllMixed_urls.txt > AllLevels_urls.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get course info\n",
    "\n",
    "urls_all = list()\n",
    "with open('AllLevels_urls.txt') as f:\n",
    "    for line in f:\n",
    "        urls_all.append(line.split('\\n')[0])\n",
    "\n",
    "course_info = {}\n",
    "\n",
    "for url in urls_all:\n",
    "\n",
    "    r  = requests.get(url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "\n",
    "    # get course title, description, syllabus headings, syllabus descriptions and add to dictionary\n",
    "    title = soup.find(class_=\"H2_1pmnvep-o_O-weightNormal_s9jwp5-o_O-fontHeadline_1uu0gyz max-text-width-xl m-b-1s\").text\n",
    "    description = soup.find_all(class_='AboutCourse')[0].find(class_=\"content-inner\").text\n",
    "    syllabus_headings_all = soup.find_all(class_='H2_1pmnvep-o_O-weightBold_uvlhiv-o_O-bold_1byw3y2 m-b-2')\n",
    "    syllabus_headings = \"\"\n",
    "    for heading in syllabus_headings_all:\n",
    "        syllabus_headings += heading.text + \" \"\n",
    "    try:\n",
    "        syllabus_descriptions_all = soup.find_all(class_='Syllabus')[0].find_all(class_=\"content-inner\")\n",
    "        syllabus_descriptions = \"\"\n",
    "        for desc in syllabus_descriptions_all:\n",
    "            syllabus_descriptions += desc.text + \" \"\n",
    "    except:\n",
    "        syllabus_descriptions = \"\"\n",
    "\n",
    "    course_info[i] = {'title': title,\n",
    "                      'description': description, \n",
    "                      'syllabus_headings': syllabus_headings, \n",
    "                      'syllabus_descriptions': syllabus_descriptions}\n",
    "    \n",
    "# save course info to dictionary\n",
    "file = open('Coursera_allinfo.pkl', 'wb')\n",
    "pickle.dump(course_info, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of reviews for each course\n",
    "\n",
    "nreviews_all = []\n",
    "for i, url in enumerate(urls_all):\n",
    "    r  = requests.get(url + '/reviews')\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "\n",
    "    try:\n",
    "        nreviews_tmp = soup.find_all(class_=\"H2_1pmnvep-o_O-weightNormal_s9jwp5-o_O-fontHeadline_1uu0gyz m-y-2 text-secondary\")[0].text\n",
    "        nreviews = int(nreviews_tmp.split('Reviews for')[0].split('of ')[1][:-1].replace(',',''))\n",
    "    except:\n",
    "        nreviews = 0\n",
    "        \n",
    "    nreviews_all.append(nreviews)\n",
    "    \n",
    "file = open('nreviews_all.pkl','wb')\n",
    "pickle.dump(nreviews_all, file)\n",
    "file.close\n",
    "\n",
    "with open(\"nreviews_all.txt\", \"w\") as file:\n",
    "    for review in nreviews_all:\n",
    "        file.write(str(review) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
