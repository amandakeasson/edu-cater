{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edu-cater "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, os, sys, json, csv, copy, operator\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, time\n",
    "import seaborn as sns\n",
    "from scipy.io import savemat, loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # ???\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.matutils import jaccard\n",
    "from gensim.matutils import jensen_shannon\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as pyldagensim\n",
    "from pyLDAvis import sklearn as pyldavis_sklearn\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import shortest_path\n",
    "\n",
    "# add scripts directory to path\n",
    "sys.path.insert(1, '../scripts/')\n",
    "from edutools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "run_pipeline = 0\n",
    "scraper = course_scraper() \n",
    "scraper.load_urls()\n",
    "\n",
    "if run_pipeline == 1:\n",
    "    scraper.scrape_urls()  \n",
    "    scraper.scrape_courses()\n",
    "    scraper.scrape_course_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing functions: tokenization, stemming, lemmatization \n",
    "# based off of code from https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    "\n",
    "stopwords = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "stopwords.extend(['youâ', 'week', 'write', 'solv', 'peer', 'assign', 'beginn',\n",
    "                  'need', 'peopl', 'content', 'teach', 'assess', 'plan', 'capston',\n",
    "                  'video', 'lesson', 'think', 'idea', 'lectur', 'learner',\n",
    "                 'quiz', 'test', 'submit', 'way', 'good', 'choos', 'begin', 'examin', 'colleg', 'academ', 'university', 'mooc',\n",
    "                 'teacher', 'educ', 'classroom', 'want', 'materi', 'instruct', 'level', 'section'])\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    stem_dict = []\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stopwords and lemmatize_stemming(token) not in stopwords and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            stem_dict.append((lemmatize_stemming(token), token))\n",
    "            \n",
    "    return result, stem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all text info (description, syllabus) and preprocess\n",
    "\n",
    "# remove courses that aren't in English (even though they had an English tag!)\n",
    "all_courses = np.arange(2635)\n",
    "bad_courses = np.array([661, 710, 919, 925, 949, 1701, 1714, 1717, 1723, 2398, 2612])\n",
    "good_courses = np.setdiff1d(all_courses, bad_courses)\n",
    "\n",
    "processed_info = []\n",
    "nreviews = []\n",
    "stars = []\n",
    "hours = []\n",
    "levels = []\n",
    "enrollment = []\n",
    "course_info_all = []\n",
    "titles_all = []\n",
    "skills_all = []\n",
    "dict_stem_full = []\n",
    "\n",
    "# some titles need to be fixed manually\n",
    "correct_titles = {1197: 'Global Financing Solutions',\n",
    "                 1672: 'Jacobi modular forms',\n",
    "                 2166: 'Bioinformatics: Introduction and Methods',\n",
    "                 2589: \"Hôtel 'De l'étoile' - a hotel in crisis\"}\n",
    "\n",
    "\n",
    "# compile all relevant course info\n",
    "for i in range(len(good_courses)):\n",
    "    \n",
    "    with open('../course_info_short/course' + str(good_courses[i]) + '.json') as json_file:\n",
    "        course_info = json.load(json_file)\n",
    "    \n",
    "    title = course_info['title']\n",
    "    if not isEnglish(title):\n",
    "        title = fix_text(title)\n",
    "    if i in correct_titles.keys():\n",
    "        title = correct_titles[i]\n",
    "    \n",
    "    titles_all.append(title) \n",
    "    allinfo = title + ' ' + fix_text(course_info['description']) \\\n",
    "            + ' ' + fix_text(course_info['syllabus_headings']) + ' ' + fix_text(course_info['syllabus_descriptions']) \n",
    "    tmpskills = ''\n",
    "    for skill in course_info['skills']:\n",
    "        skill = fix_text(skill)\n",
    "        tmpskills = tmpskills + ' ' + skill\n",
    "    for occupation in course_info['occupations']:\n",
    "        occupation = fix_text(occupation)\n",
    "        allinfo = allinfo # + ' ' + occupation\n",
    "    for review in course_info['reviews']:\n",
    "        review = fix_text(review)\n",
    "        allinfo = allinfo  # + ' ' + review\n",
    "    nreviews.append(course_info['nreviews'])\n",
    "    stars.append(course_info['stars'])\n",
    "    hours.append(course_info['hours'])\n",
    "    levels.append(course_info['level'])\n",
    "    enrollment.append(course_info['enrollment']) \n",
    "    preprocessed, stemdict = preprocess(allinfo)\n",
    "    processed_info.append(preprocessed)\n",
    "    skills_all.append(preprocess(tmpskills)[0])\n",
    "    course_info_all.append(allinfo)\n",
    "    dict_stem_full.extend(stemdict)\n",
    "    \n",
    "# save course titles for web app\n",
    "file = open('../edu-app/course_titles.pkl', 'wb')\n",
    "pickle.dump(titles_all, file)\n",
    "file.close()\n",
    "\n",
    "# save numeric info\n",
    "stars = np.array(stars)\n",
    "hours = np.array(hours)\n",
    "enrollment = np.array(enrollment)\n",
    "# fix NaNs\n",
    "hours[np.isnan(hours)==1] = np.nanmedian(hours)\n",
    "stars[np.isnan(stars)==1] = np.nanmedian(stars)\n",
    "savemat('../edu-app/course_numeric_info.mat', {'stars': stars, 'hours': hours, 'enrollment': enrollment})\n",
    "\n",
    "file = open('../data/processed_info.pkl', 'wb')\n",
    "pickle.dump([processed_info, skills_all, course_info_all], file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with a) stemmed word and \n",
    "# b) lemmatized version of word most often associated with stemmed word\n",
    "\n",
    "dict_stem_allfull = {}\n",
    "for i in range(len(dict_stem_full)):\n",
    "    if dict_stem_full[i][0] not in dict_stem_allfull.keys():\n",
    "        dict_stem_allfull[dict_stem_full[i][0]] = []\n",
    "    dict_stem_allfull[dict_stem_full[i][0]].append(dict_stem_full[i][1])\n",
    "    \n",
    "stemmed_words = list(dict_stem_allfull.keys())\n",
    "\n",
    "dict_stem_maxword = {}\n",
    "for stemmed in stemmed_words:\n",
    "    word_counts = dict(Counter(dict_stem_allfull[stemmed]))\n",
    "    maxword = max(word_counts.items(), key=operator.itemgetter(1))[0]\n",
    "    dict_stem_maxword[stemmed] = WordNetLemmatizer().lemmatize(maxword, pos='v')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for quantitative info\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "fig.add_subplot(131)\n",
    "plt.hist((np.array(stars)),20); plt.title('Rating')\n",
    "fig.add_subplot(132)\n",
    "plt.hist((hours),20); plt.title('Length (hours)')\n",
    "fig.add_subplot(133)\n",
    "plt.hist(np.log10(enrollment),20); plt.title('Enrollment (log10)')\n",
    "plt.savefig('figures/course_numeric_info.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_info)\n",
    "print('Length of original dictionary:', len(dictionary))\n",
    "\n",
    "# remove rare and common words\n",
    "dictionary.filter_extremes(no_below=10, no_above=.25, keep_n=100000)\n",
    "print('Length of filtered dictionary:', len(dictionary))\n",
    "\n",
    "# make bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_info] # 18819 3057"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine optimal # of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(ngram_range=(1,1), stop_words=stopwords, max_df=.25, min_df=10)\n",
    "clean_text = [' '.join(text) for text in processed_info]\n",
    "X = countvec.fit_transform(clean_text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jensen_shannon(components, ntopics):\n",
    "    #topic_dists = lda_model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    topic_dists = components \n",
    "    js_dists = []\n",
    "    for i in range(ntopics):\n",
    "        for j in range(ntopics):\n",
    "            if i>j:\n",
    "                js_dists.append(jensen_shannon(topic_dists[i,:], topic_dists[j,:]))\n",
    "                \n",
    "    return np.min(js_dists), np.mean(js_dists)\n",
    "\n",
    "def get_jaccard(components, ntopics):\n",
    "    topn = int(np.ceil(len(dictionary)*(10/100)))\n",
    "    topic_word_probs = components #/ model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    top_terms = np.argsort(-1*topic_word_probs,axis=1)\n",
    "    top_terms = -1*top_terms[:,0:topn]\n",
    "    jdists = []\n",
    "    for i in range(ntopics):\n",
    "        for j in range(ntopics):\n",
    "            if i > j:   \n",
    "                jdists.append(jaccard(top_terms[i,:], top_terms[j,:]))\n",
    "    return np.min(jdists), np.mean(jdists)\n",
    "\n",
    "def avg_score(components, ntopics, ytrue=None):\n",
    "    score1 = get_jensen_shannon(components, ntopics)[0]\n",
    "    score2 = get_jaccard(components, ntopics)[0]\n",
    "    return (score1 + score2)/2\n",
    "\n",
    "class LDAwithCustomScore(LatentDirichletAllocation):\n",
    "    def score(self, X, y=None):\n",
    "        # You can change the options passed to perplexity here\n",
    "        #score = avg_score(super(LDAwithCustomScore, self))\n",
    "        components = self.components_\n",
    "        ntopics = self.n_components\n",
    "        score = get_jensen_shannon(components, ntopics)[0]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV with pipeline\n",
    "\n",
    "# Define Search Param\n",
    "ntopics_list = np.arange(10,26)\n",
    "ntopics_list = [10, 15, 20]\n",
    "search_params = {'n_components': ntopics_list}\n",
    "lda = LDAwithCustomScore(random_state=0)\n",
    "\n",
    "model = GridSearchCV(lda, param_grid=search_params, cv=1)\n",
    "model.fit(X)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "file = open('../data/sklearn_model_cv_test.pkl', 'wb')\n",
    "pickle.dump(model,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('../data/sklearn_model_cv.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "ntopics_list = np.arange(10,26)\n",
    "\n",
    "cvmean = model.cv_results_['mean_test_score']\n",
    "cvstd = model.cv_results_['std_test_score']\n",
    "\n",
    "fs = 12\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(ntopics_list, cvmean, c='#00aadd')\n",
    "cvse = cvstd/np.sqrt(10)\n",
    "plt.fill_between(ntopics_list, cvmean+cvse, cvmean-cvse, alpha=.2, color='#00aadd')\n",
    "plt.legend(['Mean of CV scores', 'Std Err of CV scores'], loc='lower right')\n",
    "plt.scatter(ntopics_list, cvmean, c='#00aadd')\n",
    "plt.xlabel('# of topics', fontsize=fs)\n",
    "plt.ylabel('Jensen-Shannon Divergence',fontsize=fs)\n",
    "plt.gcf().subplots_adjust(bottom=.2)\n",
    "plt.gcf().subplots_adjust(left=.2)\n",
    "plt.savefig('figures/lda_sklearn_cv.png')\n",
    "plt.show()\n",
    "# plt.errorbar(ntopics_list, cvmean, yerr=cvse); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics_list = [10, 11, 12, 13, 14, 15]\n",
    "scores = []\n",
    "for ntopics in ntopics_list:\n",
    "    print(ntopics)\n",
    "    lda_model = LDAwithCustomScore(random_state=0, n_components=ntopics)\n",
    "    lda_model.fit_transform(X)\n",
    "    scores.append(lda_model.score(X))\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-define model using optimal # of topics, and print topics\n",
    "\n",
    "ntopics = 14\n",
    "myseed = 0\n",
    "pkg = 'sklearn'\n",
    "\n",
    "if pkg == 'sklearn':\n",
    "    lda_model = LDAwithCustomScore(random_state=myseed, n_components=ntopics)\n",
    "    lda_model.fit_transform(X);\n",
    "    data = pyldavis_sklearn.prepare(lda_model, X, countvec, mds = 'tsne', sort_topics=False)\n",
    "elif pkg == 'gensim':\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, num_topics = ntopics, id2word = dictionary, \n",
    "                                        random_state=myseed, passes = 10, workers = 2, iterations=300)\n",
    "    data = pyldagensim.prepare(lda_model, bow_corpus, dictionary, mds='tsne', sort_topics=False)\n",
    "    \n",
    "# fix terms to be lemmatized most commonword instead of stemmed word\n",
    "#for row in range(len(data.topic_info.index)):\n",
    "#    if row%50 == 0:\n",
    "#        print(row)\n",
    "#    maxword = dict_stem_maxword[data.topic_info[\"Term\"].iloc[row]]\n",
    "#    data.topic_info[\"Term\"].iloc[row] = maxword\n",
    "\n",
    "#pyLDAvis.save_html(data,'../edu-app/static/courseviz.html')\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniqueness of topics\n",
    "\n",
    "Use Jensen-Shannon divergence and Jaccard distance to determine how unique the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard\n",
    "topn = int(np.ceil(len(dictionary)*(10/100)))\n",
    "topic_word_probs = lda_model.components_\n",
    "top_terms = np.argsort(-1*topic_word_probs,axis=1)\n",
    "top_terms = -1*top_terms[:,0:topn]\n",
    "jdists = np.zeros((ntopics,ntopics))\n",
    "for i in range(ntopics):\n",
    "    for j in range(ntopics):  \n",
    "        jdist = jaccard(top_terms[i,:], top_terms[j,:])\n",
    "        jdists[i,j] = jdist\n",
    "\n",
    "# jensen shannon distance\n",
    "js_dists = np.zeros((ntopics,ntopics))\n",
    "topic_dists = lda_model.components_\n",
    "for i in range(ntopics):\n",
    "    for j in range(ntopics):\n",
    "        js_dists[i,j] = jensen_shannon(topic_dists[i,:], topic_dists[j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "\n",
    "mask = np.zeros_like(jdists, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "uptri_jdists = np.triu(jdists)\n",
    "uptri_jdists = uptri_jdists[uptri_jdists>0]\n",
    "print('Mean Jaccard distance:', np.mean(uptri_jdists))\n",
    "\n",
    "\n",
    "uptri_js_dists = np.triu(js_dists)\n",
    "uptri_js_dists = uptri_js_dists[uptri_js_dists>0]\n",
    "print('Mean JS distance:', np.mean(uptri_js_dists))\n",
    "\n",
    "\n",
    "def label_cbar(ax):\n",
    "    cb = ax.collections[0].colorbar\n",
    "    cb.ax.text(0.5, -0.1, 'Less unique', transform=cb.ax.transAxes, \n",
    "        va='top', ha='center', fontsize=12)\n",
    "    cb.ax.text(0.5, 1.05, 'More unique', transform=cb.ax.transAxes, \n",
    "        va='bottom', ha='center', fontsize=12)\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "ax = sns.heatmap(jdists, mask=mask, cmap='Reds', vmin=np.round(np.min(uptri_jdists),2),\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "tick_labels = ['Topic ' + str(topic) for topic in np.arange(ntopics)+1]\n",
    "ax.set_xticklabels(tick_labels, rotation=90)\n",
    "ax.set_yticklabels(tick_labels, rotation=0)\n",
    "label_cbar(ax)\n",
    "plt.title('Jaccard Distance (top 10% of terms)',fontsize=16)\n",
    "\n",
    "fig.add_subplot(122)\n",
    "ax = sns.heatmap(js_dists, mask=mask, cmap='Reds', vmin=np.round(np.min(uptri_js_dists),2),\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "ax.set_xticklabels(tick_labels, rotation=90)\n",
    "ax.set_yticklabels(tick_labels, rotation=0)\n",
    "label_cbar(ax)\n",
    "plt.title('Jensen-Shannon Divergence', fontsize=16)\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=.2)\n",
    "plt.savefig('figures/topic_uniqueness.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between topic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity b/w topic scores \n",
    "    \n",
    "scoremat = lda_model.transform(X)\n",
    "savemat('../edu-app/scoremat.mat',{'scoremat': scoremat})\n",
    "\n",
    "scorecorrs = cos_sim(scoremat)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.imshow(scorecorrs, cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of best-matching topics\n",
    "\n",
    "# this is how freqs are calculated in pyLDAvis code\n",
    "doc_lengths = X.sum(axis=1).getA1()\n",
    "doc_topic_dists = lda_model.transform(X)\n",
    "topic_freq = np.dot(doc_topic_dists.T, doc_lengths) # (doc_topic_dists.T * doc_lengths).T.sum()\n",
    "topic_proportion = (topic_freq / topic_freq.sum())\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.bar(np.arange(ntopics)+1, topic_proportion, color=\"#00aadd\")\n",
    "my_x_labels = []\n",
    "for x in range(ntopics):\n",
    "    my_x_labels.append('Topic ' + str(x+1))\n",
    "plt.xticks(np.arange(ntopics)+1,my_x_labels, rotation=90)\n",
    "#plt.title('Topic Distribution')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Marginal Topic Distribution')\n",
    "plt.gcf().subplots_adjust(bottom=.3)\n",
    "plt.savefig('figures/topicdist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph theory with networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph theory with networkx\n",
    "\n",
    "scorecorrs = cos_sim(scoremat)\n",
    "scorecorrs_triu = np.triu(scorecorrs)\n",
    "scorecorrs_triu = scorecorrs_triu[scorecorrs_triu>0]\n",
    "thresh = np.percentile(scorecorrs_triu,92.5)\n",
    "print(thresh)\n",
    "# thresh=.7\n",
    "scorecorrs[scorecorrs<=thresh] = 0\n",
    "\n",
    "strength = np.sum(scorecorrs,0)\n",
    "print('min strength:', np.min(strength))\n",
    "\n",
    "G = nx.from_numpy_matrix(scorecorrs)\n",
    "#pos = nx.spring_layout(G)\n",
    "weights = dict(G.degree(weight='weight'))\n",
    "values = [weights.get(node, 0.25) for node in G.nodes()]\n",
    "values_lda = copy.deepcopy(values)\n",
    "values = np.argmax(doc_topic_dists,axis=1)\n",
    "\n",
    "file = open('../edu-app/networkx_graph.pkl','wb')\n",
    "pickle.dump(G, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../edu-app/networkx_pos.pkl', 'wb')\n",
    "pickle.dump(pos, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../edu-app/networkx_values.pkl', 'wb')\n",
    "pickle.dump(values, file)\n",
    "file.close()\n",
    "\n",
    "with open('../edu-app/static/nodes_orig.csv', mode='w') as fp:\n",
    "    fwriter = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow(['x', 'y', 'strength', 'radius','title'])\n",
    "    for i in range(len(pos)):\n",
    "        fwriter.writerow([pos[i][0], pos[i][1], int(values[i]), 2, titles_all[i]])\n",
    "\n",
    "plt.figure(1,figsize=(10,10)) \n",
    "nx.draw(G, pos, node_size=20, node_color = values, width=.1, cmap='GnBu', edge_color = \"#222222\")\n",
    "plt.savefig(\"figures/coursera_lda_network.png\", format=\"PNG\")\n",
    "plt.show(block=False)\n",
    "\n",
    "# write edges.csv: saved edges are for visualization only (higher threshold)\n",
    "list_edges = list(G.edges())\n",
    "with open('../edu-app/static/edges_orig.csv', mode='w') as fp:\n",
    "    fwriter = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow(['x1', 'x2', 'y1', 'y2', 'width', 'color'])\n",
    "    for i in range(len(list_edges)):\n",
    "        if list_edges[i][0] != list_edges[i][1]:\n",
    "            x1 = pos[list_edges[i][0]][0]\n",
    "            x2 = pos[list_edges[i][1]][0]\n",
    "            y1 = pos[list_edges[i][0]][1]\n",
    "            y2 = pos[list_edges[i][1]][1]\n",
    "            fwriter.writerow([x1, x2, y1, y2, G.edges[list_edges[i][0],list_edges[i][1]]['weight'], '#000000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.percentile(scorecorrs_triu,97.5)\n",
    "scorecorrs = cos_sim(scoremat)\n",
    "scorecorrs[scorecorrs<=thresh] = 0\n",
    "strength = np.sum(scorecorrs, axis=0)\n",
    "strength_thresh = 15\n",
    "inds = np.where(strength<=strength_thresh)[0]\n",
    "print(len(inds))\n",
    "\n",
    "G = nx.from_numpy_matrix(scorecorrs)\n",
    "for ind in inds:\n",
    "    G.remove_node(ind)\n",
    "\n",
    "graphs = list(nx.connected_component_subgraphs(G))\n",
    "graph_sizes = []\n",
    "for graph in graphs:\n",
    "    graph_sizes.append(len(graph.nodes()))\n",
    "    \n",
    "best_ind = np.where(graph_sizes==np.max(graph_sizes))[0][0]\n",
    "bestgraph = graphs[best_ind]\n",
    "\n",
    "G = bestgraph \n",
    "\n",
    "values_thresh = copy.deepcopy(np.array(values))\n",
    "#values_thresh = values_thresh[np.where(strength>strength_thresh)[0]]\n",
    "newnodes = list(bestgraph.nodes())\n",
    "values_thresh = values_thresh[newnodes]\n",
    "\n",
    "plt.figure(1,figsize=(10,10)) \n",
    "nx.draw(G, pos, node_size=20, node_color = values_thresh, width=.1, cmap='GnBu', vmin = np.min(values), edge_color=\"#222222\")\n",
    "plt.savefig(\"figures/coursera_lda_network_thresh.png\", format=\"PNG\")\n",
    "plt.show(block=False)\n",
    "\n",
    "# write nodes for plotting (viz only)\n",
    "with open('../edu-app/static/nodes_orig_plot.csv', mode='w') as fp:\n",
    "    fwriter = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow(['x', 'y', 'strength', 'radius','title'])\n",
    "    for i in range(len(pos)):\n",
    "        if i in G.nodes():\n",
    "            fwriter.writerow([pos[i][0], pos[i][1], int(values[i]), 2, titles_all[i]])\n",
    "\n",
    "# write edges.csv: saved edges are for visualization only (higher threshold)\n",
    "list_edges = list(G.edges())\n",
    "with open('../edu-app/static/edges_orig_plot.csv', mode='w') as fp:\n",
    "    fwriter = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow(['x1', 'x2', 'y1', 'y2', 'width', 'color'])\n",
    "    for i in range(len(list_edges)):\n",
    "        if list_edges[i][0] != list_edges[i][1]:\n",
    "            x1 = pos[list_edges[i][0]][0]\n",
    "            x2 = pos[list_edges[i][1]][0]\n",
    "            y1 = pos[list_edges[i][0]][1]\n",
    "            y2 = pos[list_edges[i][1]][1]\n",
    "            fwriter.writerow([x1, x2, y1, y2, G.edges[list_edges[i][0],list_edges[i][1]]['weight'], '#000000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare to coursera's recommendation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make binary graph: rows = course being viewed; cols = courses that are recommended\n",
    "ncourses = len(titles_all)\n",
    "coursenet_full = np.zeros((ncourses,ncourses))\n",
    "for i in range(ncourses):\n",
    "    mat = loadmat('../course_nets/course'+str(good_courses[i])+'.mat')\n",
    "    coursenet = mat['coursenet']\n",
    "    coursenet = coursenet[:,good_courses][0]\n",
    "    coursenet_full[i,:] = coursenet\n",
    "    inds = np.where(coursenet_full[i,:]==1.0)[0]\n",
    "    \n",
    "    \n",
    "# topic-topic similarity for courses that are recommended by coursera\n",
    "coursenet_sim = np.zeros((ncourses,ncourses))\n",
    "for i in range(ncourses):\n",
    "    for j in range(ncourses):\n",
    "        scores1 = scoremat[i,:]\n",
    "        scores2 = scoremat[j,:]\n",
    "        if coursenet_full[i,j] == 1:\n",
    "            coursenet_sim[i,j] = cos_sim(scores1[:,np.newaxis].T,scores2[:,np.newaxis].T)[0][0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.percentile(scorecorrs_triu,92.5)\n",
    "coursenet_thresh = copy.deepcopy(coursenet_sim)\n",
    "#coursenet_thresh[coursenet_thresh<=thresh] = 0\n",
    "#coursenet_thresh[coursenet_thresh>thresh] = 1\n",
    "\n",
    "G = nx.from_numpy_matrix(coursenet_thresh)\n",
    "nx.from_numpy_matrix\n",
    "weights = dict(G.degree(weight='weight'))\n",
    "values = np.array([weights.get(node, 0.25) for node in G.nodes()])\n",
    "values_recs = copy.deepcopy(values)\n",
    "print(np.sum(values==1))\n",
    "file = open('../edu-app/networkx_pos.pkl','rb')\n",
    "pos = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "pos_new = nx.spring_layout(G, pos=pos, iterations=2, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8, 8)) \n",
    "nx.draw(G, pos_new, node_size=20, node_color=values, width=.1, cmap='GnBu', vmin=-30)\n",
    "plt.savefig(\"figures/coursera_rec_network.png\", format=\"PNG\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare node strength for lda and orig recs\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "fig.add_subplot(121)\n",
    "plt.hist(values_lda)\n",
    "plt.title('Node Strength: edu-cater', fontsize=16)\n",
    "fig.add_subplot(122)\n",
    "plt.hist(values_recs)\n",
    "plt.title('Node Strength: Coursera', fontsize=16)\n",
    "plt.savefig('figures/node_strength.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the LDA model better than Coursera's current recommendation system?\n",
    "\n",
    "1) The cosine similarity between topic scores for a document (used to train the model) and topic scores from skills tags (not used in model training) is high. Thus, the topics generated from LDA are valid.  \n",
    "\n",
    "2) However, for a given course, the overlap between the skills tags and the top 10 terms from the most relevant topic is low (max 4/10). Thus, the topics provide more information than the skills tags alone.  \n",
    "\n",
    "These features are demonstrated with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_skills_scores(ind): \n",
    "    \n",
    "    unseen_document = course_info_all[ind]\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document)[0])\n",
    "    topic_tuples = lda_model.get_document_topics(bow_vector, 0, 0, True)[0]\n",
    "    topic_scores = np.zeros((1, len(topic_tuples)))\n",
    "    for i, score in enumerate(topic_tuples):\n",
    "        topic_scores[0,i] = score[1]\n",
    "        \n",
    "    course_topic_scores = topic_scores\n",
    "        \n",
    "    unseen_document = skills_all[ind]\n",
    "    bow_vector = dictionary.doc2bow(unseen_document)\n",
    "    topic_tuples = lda_model.get_document_topics(bow_vector, 0, 0, True)[0]\n",
    "    topic_scores = np.zeros((1, len(topic_tuples)))\n",
    "    for i, score in enumerate(topic_tuples):\n",
    "        topic_scores[0,i] = score[1]\n",
    "        \n",
    "    skills_topic_scores = topic_scores\n",
    "        \n",
    "    return course_topic_scores, skills_topic_scores\n",
    "\n",
    "course_skills_sim = []\n",
    "topterms_in_skills = []\n",
    "\n",
    "\n",
    "def get_course_skills_sim():\n",
    "    for i in range(len(skills_all)):\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "\n",
    "        # not all courses have skills tags\n",
    "        if len(skills_all[i])>0:\n",
    "            course_scores, skills_scores = get_course_skills_scores(i)\n",
    "            cs_sim = cos_sim(course_scores, skills_scores)[0][0]\n",
    "            course_skills_sim.append(cs_sim)\n",
    "\n",
    "            skills = set(skills_all[i])\n",
    "            best_topic = np.argmax(course_scores)\n",
    "            topic_terms = lda_model.get_topic_terms(best_topic, topn=30)\n",
    "            top_terms = []\n",
    "            for x in range(len(topic_terms)):\n",
    "                top_terms.append(dictionary[topic_terms[x][0]])\n",
    "\n",
    "            counter = 0\n",
    "            for term in top_terms:\n",
    "                if term in skills:\n",
    "                    counter +=1\n",
    "            topterms_in_skills.append(counter)\n",
    "        \n",
    "    return topterms_in_skills\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_skills_scores(ind): \n",
    "    \n",
    "    unseen_document = course_info_all[ind]\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document)[0])\n",
    "    print(bow_vector)\n",
    "    topic_tuples = lda_model.get_document_topics(bow_vector, 0, 0, True)[0]\n",
    "    topic_scores = np.zeros((1, len(topic_tuples)))\n",
    "    for i, score in enumerate(topic_tuples):\n",
    "        topic_scores[0,i] = score[1]\n",
    "        \n",
    "    course_topic_scores = lda_model.transform(bow_vector)\n",
    "        \n",
    "    unseen_document = skills_all[ind]\n",
    "    bow_vector = dictionary.doc2bow(unseen_document)\n",
    "    #topic_tuples = lda_model.get_document_topics(bow_vector, 0, 0, True)[0]\n",
    "    #topic_scores = np.zeros((1, len(topic_tuples)))\n",
    "    #for i, score in enumerate(topic_tuples):\n",
    "    #    topic_scores[0,i] = score[1]\n",
    "        \n",
    "    skills_topic_scores = lda_model.transform(bow_vector)\n",
    "        \n",
    "    return course_topic_scores, skills_topic_scores\n",
    "\n",
    "\n",
    "course_skills_sim = []\n",
    "topterms_in_skills = []\n",
    "\n",
    "for i in range(len(skills_all)):\n",
    "    \n",
    "    # not all courses have skills tags\n",
    "    if len(skills_all[i])>0:\n",
    "        course_scores, skills_scores = get_course_skills_scores(i)\n",
    "        cs_corr = np.corrcoef(course_scores, skills_scores)[0,1]\n",
    "        cs_sim = cos_sim(course_scores, skills_scores)[0][0]\n",
    "        course_skills_sim.append(cs_sim)\n",
    "        \n",
    "        skills = set(skills_all[i])\n",
    "        best_topic = np.argmax(course_scores)\n",
    "        topic_terms = lda_model.get_topic_terms(best_topic, topn=30)\n",
    "        top_terms = []\n",
    "        for x in range(len(topic_terms)):\n",
    "            top_terms.append(dictionary[topic_terms[x][0]])\n",
    "            \n",
    "        counter = 0\n",
    "        for term in top_terms:\n",
    "            if term in skills:\n",
    "                counter +=1\n",
    "        topterms_in_skills.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots to show similarity and overlap\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "fig.add_subplot(121); plt.hist(course_skills_sim); \n",
    "plt.title('Cosine Similarity of Topic Scores \\n for Documents and Skills Tags')\n",
    "fig.add_subplot(122); plt.hist(topterms_in_skills,np.max(topterms_in_skills)+1);\n",
    "plt.xticks(np.arange(np.max(topterms_in_skills)+1))\n",
    "plt.title('# of topic terms in skills tags')\n",
    "plt.gcf().subplots_adjust(top=.80)\n",
    "plt.savefig('figures/validation1.png')\n",
    "plt.show()\n",
    "print(len(course_skills_sim), 'courses had skills tags')\n",
    "print('Median similarity between document scores and topic scores:',np.median(course_skills_sim))\n",
    "print('Median number of skills tag terms present in top 10 terms:',np.median(topterms_in_skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example graph to illustrate shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = nx.Graph()\n",
    "g2.add_edge(4, 2, weight=5)\n",
    "g2.add_edge(2, 3, weight=2)\n",
    "g2.add_edge(4, 5, weight=3)\n",
    "g2.add_edge(2, 1, weight=1)\n",
    "g2.add_edge(3, 5, weight=1)\n",
    "\n",
    "strength = dict(g2.degree(weight='weight'))\n",
    "values = [strength.get(node, 0.25) for node in g2.nodes()]\n",
    "weights = [g2[u][v]['weight'] for u,v in g2.edges]\n",
    "\n",
    "pos = nx.spring_layout(g2, weight=None)\n",
    "\n",
    "fig = plt.figure(figsize=(4,2))\n",
    "nx.draw(g2, pos, width=weights, node_size=1200, node_color=values, cmap='jet', vmin=-3, vmax=10, with_labels=True)\n",
    "shortest_path(g2,1,5,weight=\"weight\")\n",
    "plt.savefig('figures/shortpath_example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.ones((5,5))\n",
    "tmp[0:len(tmp), 0:len(tmp)] = 0\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
